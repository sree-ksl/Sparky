object Mlyelp {

  object Config {
    @Parameter(names = Array("-st", "--slackToken"))
    var slackToken: String = null
    @Parameter(names = Array("-nc", "--numClusters"))
    var numClusters: Int = 4
    @Parameter(names = Array("-po", "--predictOutput"))
    var predictOutput: String = null
    @Parameter(names = Array("-td", "--trainData"))
    var trainData: String = null
    @Parameter(names = Array("-ml", "--modelLocation"))
    var modelLocation: String = null

  }
  def main(args: Array[String]) {
    new JCommander(Config, args.toArray: _*)
    val conf = new SparkConf().setAppName("StreamingWithML")
    val sparkContext = new SparkContext(conf)
    // optain existing or create new model
    val clusters: KMeansModel =
    if (Config.trainData != null) {
      KMeanTrainTask.train(sparkContext, Config.trainData, Config.numClusters, Config.modelLocation)
    } else {
      if (Config.modelLocation != null) {
        new KMeansModel(sparkContext.objectFile[Vector](Config.modelLocation).collect())
      } else {
        throw new IllegalArgumentException("Either modelLocation or trainData should be specified")
      }
    }
    if (Config.slackToken != null) {
      StreamingTask.run(sparkContext, Config.slackToken, clusters, Config.predictOutput)
    }
  }

  def train(sparkContext: SparkContext, trainData: String, numClusters: Int, modelLocation: String): KMeansModel = {
    if (new File(modelLocation).exists) removePrevious(modelLocation)
    val trainRdd = sparkContext.textFile(trainData)
    val parsedData = trainRdd.map(Utils.featurize).cache()
    // if we had a really large data set to train on, we'd want to call an action to trigger cache.
    val model = KMeans.train(parsedData, numClusters, numIterations)
    sparkContext.makeRDD(model.clusterCenters, numClusters).saveAsObjectFile(modelLocation)
    val example = trainRdd.sample(withReplacement = false, 0.1).map(s => (s, model.predict(Utils.featurize(s)))).collect()
    println("Prediction :")
    example.foreach(println)
    model
  }
}